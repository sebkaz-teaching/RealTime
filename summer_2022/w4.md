---
layout: page
title: Wykład 4
mathjax: true
---

## Algorytmy

Zanim zaprojektujesz rozwiązanie problemu biznesowego warto zastanowic się nad złożonością 
Twojego problemu.

1. **Algorytmy przetwarzające duże ilości danych**  - przetwarzanie gigantycznych zbiorów danych. Uproszczone wymagania dotyczące procesowania. W przypadku gdy dane przekraczają iloś pamięci jednostki obliczeniowej często korzysta się z iteracyjnego sposobu przetwarzania danych. 
2. **Algorytmy dokonujące wielu obliczeń** - wymagają dużej mocy obliczeniowej. Zazwyczaj nie operują na dużych zbiorach danych (np algorytm znajdujący dużą liczbę pierwszą). Wykorzystuje się tutaj podział elementów algorytmu na takie, które można zrównoleglic. 
3. **Algorytmy przetwarzające duże ilości danych i dokonujące wielu obliczeń** Wymagają dużych zasobów obliczeniowych i realizowane są na dużych danych. np. analiza sentymentu w plikach wideo udostępnianych na żywo.

### Wymiar danych
Aby określic wymiar danych problemu nie wystarczy podac ilośc miejsca zajmowanego przez dane. 
1. Rozmiar wejścia - oczekiwany rozmiar danych do przetwarzania
2. Szybkośc narastania - oczekiwane tempo generowania nowych danych podczas uzywania algorytmu.
3. Różnorodnośc struktury - oczekiwane typy danych

### Wymiar obliczeniowy
Dotyczy zasobór procesowania i mocy obliczeniowej. np Algorytmy uczenia głębokiego wymagają dużej mocy obliczeniowej dlatego warto zapewnic zrównolegloną architekturę bądź zawierającą GPU lub TPU. 

## Wyjaśnienie algorytmów 

W wielu sytuacjach modelowanie używane jest w tzw. przypadkach krytycznych np . oprogramowanie podające leki. 
W takich stuacjach istotne staje się wyjaśnienie przyczyny pojawienia się każdego rezultatu działania naszego algorytmu. Jest to istotne i konieczne by mie pewnośc, że decyzje podejmowane na podstawie algorytmu są wolne od przekłamań. Zdolnośc dokładnego wskazania mechanizmów, które zachodzą bezpośrednio lub pośrednio przy generowaniu rezultatu nazywamy **możliwością wyjaśnienia** algorytmu. Analiza etyczna jest standardem procesu walidacji algorytmu. 
Zdolnośc tą bardzo trudno uzyskac dla algorytmów ML i DL. Jeśli bank korzysta z algorytmu do podjęcia decyzji kredytowej musi byc transparentny i zawsze wskazac powody uzyskanej decyzji. Jedną z technik jest metoda LIME (*Local Interpretable Model-Agnostic Explonations*) opublikowana w 2016 roku. Opiera się ona na wprowadzeniu małej zmiany w danych wejściowych w każdym egzemplarzu, a następnie próbie określenia lokalnych ram decyzji dla tego egzemplarza. 

## Detekcja anomalii 

**Wartość odstająca** (ang. outlier) to obserwacja (wiersz w tabeli danych), która jest względnie odległa od pozostałych elementów próby. Wyraża ona przekonanie, iż związek między zmiennymi niezależnymi i zależnymi dla danej obserwacji może być inny niż dla pozostałych obserwacji.
Dla pojedynczych zmiennych wartości odstające można określić wykorzystując wykres pudełkowy. Realizuje on wartości kwartyli, gdzie pierwszy i trzeci wyznaczają boki, natomiast wewnątrz umieszczana jest linia podziału realizująca drugi kwartyl (czyli medianę). Jego generowanie pozwala nam opisać rozkład oraz zaznaczyć wartości nietypowe jako spełniające $x_{out} < Q_{1/4} - 3Q$ lub $x_{out} > Q_{3/4} + 3Q$ gdzie $Q = frack{Q_{3/4}-Q_{1/4}}{2}$.

Pojęcie wartości odstającej jest dość intuicyjne. Bolid formuły 1, jeśli chodzi o prędkość, to samochodowy outlier pośród samochodów używanych na co dzień.

W przypadku zagadnień z uczeniem nadzorowanym usuwanie anomalnych danych często poprawia jakość otrzymanego modelu.
Poszukiwanie wartości odstających wykorzystywane jest również w procesie **detekcji anomalii**. 

Polega on na wyszukiwaniu „dziwnych” przypadków w zbiorze danych. 

Przetwarzając zbiór danych transakcji kredytowych technika ta może być wykorzystana do określenia transakcji fraudowych. Ma ona również zastosowanie w:
-  wyszukiwaniu intruzów w sieci internetowej poprzez analizę zachowań jej użytkowników, 
-  monitorowaniu danych medycznych 
-  wyszukiwaniu wadliwych elementów poprzez analizę obrazu. 

Metody wyszukiwania anomalii możemy podzielić ze względu na wykorzystywane modele uczenia maszynowego, na nadzorowane i nienadzorowane. 

1. Do nadzorowanych metod można zaliczyć:
- sieci neuronowe, 
- algorytm K–najbliższych sąsiadów 
- sieci Bayesowskie. 

2. W przypadku metod nienadzorowanych najczęściej bazuje się na założeniu, że większość napływających danych jest prawidłowa i tylko bardzo niewielki procent to dane odstające. 
Wykorzystuje się tutaj takie metody jak 
- klasteryzacja metodą K–średnich, 
- autoenkodery
- metody wykorzystujące testowanie hipotez. 

Oczywiście metod wyszukiwania anomalii można znaleźć dużo więcej niż tylko wymienione wyżej.

### Metoda klasyczna

Jej zasadniczym elementem pozwalającym stwierdzić czy dany przypadek jest anomalią to prawdopodobieństwo pojawienia się danego wiersza danych $p(x)$. Jeśli jest ono małe $p(x)<\epsilon$ to można taką wartoś traktowa jako anomalie.

Jak policzyc prawdopodopbieństwo otrzymania danego punktu ? - **określ rozkład**. 

>>> Postać rozkładu musimy założyć.

Najprościej jest przyjąć, iż rozkład ten jest rozkładem normalnym $N(\mu,\sigma)$. 
Jeśli nasze dane zawierają więcej niż jedną cechę możemy zastosować naiwne założenie, iż zmienne te są od siebie niezależne. Wartości parametrów rozkładu wyestymować możemy z próby zgodnie z wyborem odpowiednich estymatorów. 

Wartość oczekiwana estymowana jest przez średnią z próby, natomiast wariancję możemy wyestymować jako wariancję z próby. 

Po otrzymaniu tych wartości dla każdej zmiennej odczytujemy, np. z tablic rozkładu normalnego, prawdopodobieństwo pojawienia się konkretnej wartości $x_i$. 

Ewaluacje wybranej metodologii można przeprowadzić poprzez wygenerowanie modelu klasyfikacji binarnej dla naszych danych. 

Wskazany przykład można uznać za realizację modelowania anomalii z wykorzystaniem uczenia nadzorowanego.


### Isolation Forest 

Isolation Forest to algorytm oparty na algorytmie drzewa decyzyjnego, który identyfikuje wartości odstające (anomalie) poprzez izolację.

Zaproponowany został przez Fei Tony Liu, Kai Ming Ting oraz Zhi-Hua Zhou w 2008 roku.

Algorytm ten ma liniową złożoność obliczeniową i pozwala na szybką detekcję anomalii z wykorzystaniem niedużej ilości pamięci oraz działa dobrze w przypadku wielowymiarowych danych.

Izolacja wartości odstających następuje poprzez losowy wybór zmiennej z całego zbioru zmiennych i losowego wyboru punktu podziału (pomiędzy wartością minimalną i maksymalną). Losowy podział generuje mniejszą drogę w drzewie w przypadku anomalnych wartości (znajdują się one bliżej korzenia drzewa) niż dla wartości typowych.

 Odległość tą można wyznaczyć generując dużą ilość drzew, a następnie obliczając średnią odległość od korzenia, gdyż każdy punkt przechodząc przez drzewo generuje wynik na jakiejś głębokości drzewa.

 [Metody detekcji anomalii sklearn](https://scikit-learn.org/0.20/auto_examples/plot_anomaly_comparison.html)