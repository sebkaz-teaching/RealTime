{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# California Housing Prices\n",
    "\n",
    "Dane na podstawie spisu ludności Kalifornii z 1990 roku, część repozytorium StatLib. \n",
    "\n",
    "![](Mieszkania.pdf)\n",
    "\n",
    "## Zadanie - Stwórz model cen mieszkań w Kalifornii\n",
    "\n",
    "Posiadając dane o stanie populacji, medianie dochodów, medianie cen mieszkań itd stwórz model przewidujący medianę cen mieszkań w dowolnym dystrykcie. Przez dystrykt rozumie się poszczególną grupę bloków (600 do 3000 osób)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modele, modele, modele...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Zanim przeliczysz wszystko przemyśl problem oraz określ jego zakres\n",
    "\n",
    "Na jakie pytanie odpowiesz w pierwszej kolejności ? \n",
    "\n",
    "### Jaki jest dokładny cel biznesowy ? \n",
    "\n",
    "Jak chcesz skorzystać i zarobić na tym modelu ? \n",
    "\n",
    "W ramach tego zadania otrzymujesz odpowiedź od szefa, iż wynik modelu (prognoza mediany cen mieszkań) ma zostać przekazany do innych algorytmów dzięki którym Twój szef otrzyma odpowiedź na pytanie czy warto inwestować w danym obszarze. \n",
    "\n",
    "\n",
    "### Jak wygląda obecne rozwiązanie problemu w firmie\n",
    "\n",
    "Czy w ogóle jakieś istnieje ? Jeśli tak to jaka jest wydajność. Co jeśli dowiadujesz się, że proces jest bardzo skomplikowany i obecnie medianę ceny określa grono ekspertów ? Błędy w określeniu mediany sięgają 10%.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projektowanie systemu\n",
    "\n",
    "1. Jak sądzisz ? jakiego typu uczenie nada się dla tego problemu ? nadzorowane, nienadzorowane czy może uczenie przez wzmacnianie ? \n",
    "\n",
    "2. Jest to zadanie klasyfikacji, regresji czy jakieś inne ? \n",
    "\n",
    "3. Uczenie przyrostowe czy wsadowe ? \n",
    "\n",
    "4. Co oznacza termin `multivariate regression`?\n",
    "\n",
    "5. Jaką metrykę wydajności zaproponujesz ? \n",
    "\n",
    "6. Wyjaśnij skrót RMSE. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uwaga !\n",
    "\n",
    "W sytuacji kiedy masz wiele wartości odstających zamiast RMSE  lepiej wybrać `Średni absolutny błąd (Mean Absolute Error)` jako funkcję kosztu. Obydwie miary pozwalają obliczyć odległość między dwoma wektorami: wektorem prognoz i wektorem wartości docelowych. Reprezentują one tzw. `normę`. RMSE związana jest z normą euklidesową (często nazywana normą $l_2$). Natomiast MAE to norma $l_1$ czyli tzw norma taksówkowa, miejska bądź Manhattan. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sprawdź założenia\n",
    "\n",
    "Przed wpisywaniem czegokolwiek do komputera warto zweryfikować dotychczasowe założenia. Chciałbyś zobaczyć minę swojego szefa gdy Twój model będzie przewidywał konkretną wartość a do systemu ma być przekazywana jedna z wartości: \"tani\", \"średni\", \"drogi\". Jak zmieniłoby to Twoje modelowanie ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wreszcie kod\n",
    "\n",
    "### Przestrzeń robocza\n",
    "\n",
    "1. Środowisko Python3. Jeśli jeszcze nie masz [Python3](https://www/python.org)\n",
    "\n",
    "2. Katalog roboczy do którego skopiowałeś ten notebook oraz plik z danymi.\n",
    "\n",
    "```{bash}\n",
    "export ML_PATH=\"$HOME/projekt1\"\n",
    "mkdir -p $ML_PATH\n",
    "``` \n",
    "\n",
    "3. Potrzebne są dla nas następujące moduły (szczególnie jeśli dodałeś środowisko wirtualne do swojego projektu): Jupyter, NumPy, Pandas, Matplotlib i Scikit-Learn. \n",
    "\n",
    "```{bash}\n",
    "pip --version\n",
    "pip3 --version\n",
    "\n",
    "pip3 install --upgrade pip \n",
    "\n",
    "# jeśli masz virtualenv\n",
    "pip3 install --upgrade virtualenv\n",
    "# stworz środowisko\n",
    "cd $ML_PATH\n",
    "virtualenv env\n",
    "# a jeśli chcesz uruchomić to \n",
    "cd $ML_PATH\n",
    "source env/bin/activate\n",
    "\n",
    "# instalacja modułów\n",
    "pip3 install --upgrade jupyter matplotlib numpy pandas scipy scikit-learn\n",
    "# weryfikacja czy działa\n",
    "python3 -c \"import jupyter, matplotlib, numpy, pandas, scipy, sklearn\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dane\n",
    "\n",
    "W typowym zadaniu dane najczęściej będą umieszczone w relacyjnej bazie danych. Dostęp do baz danych wymaga uzyskania informacji o autoryzacji oraz info o schemacie dancyh. W naszym przypadku wszystko czego potrzeba zamieszczone jest w pliku `housing.csv`. Ze względu, iż nie jesteś leniwy napisz krótki skrypt pobierający dane z repo git."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "\n",
    "DOWNLOAD_ROOT = 'https://raw.githubusercontent.com/ageron/handson-ml/master/'\n",
    "HOUSING_PATH = os.path.join(\"zestaw danych\",\"mieszkania\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    if not os.path.isdir(housing_path):\n",
    "        os.makedirs(housing_path)\n",
    "        tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "        urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "        housing_tgz = tarfile.open(tgz_path) \n",
    "        housing_tgz.extractall(path=housing_path) \n",
    "        housing_tgz.close()\n",
    "\n",
    "fetch_housing_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do załadowania danych także napiszemy prostą funkcję "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, 'housing.csv')\n",
    "    return pd.read_csv(csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_housing_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wyświetl 5 pierwszych wierszy danych\n",
    "\n",
    "Każdy wiersz to jeden dystrykt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wypisz nazwy atrybutów "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ile jest wszystkich danych ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Czy są jakieś braki danych ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jakie typy danych są w zbiorze ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jakie kategorie i ile elementów jest w danej kategorii dla zmiennej `ocean_proximity` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wygeneruj podsumowanie wszystkich atrybutów numerycznych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Narysuj histogramy wszystkich zmiennych "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jakie wnioski możesz wyciągnąć na podstawie histogramów ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podziel dane na zbiór treningowy i testowy\n",
    "\n",
    "Teoretycznie tworzenie zbioru testowego to trywialna sprawa - wystarczy losowo wybrać część przykładów (np 20%)i zapisać rozdzielone zbiory do dwóch ramek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def split_train_test(data, test_ratio):\n",
    "    shuffled_indices = np.random.permutation(len(data)) \n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size] \n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uczące: 16512 , testowe: 4128\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = split_train_test(df, 0.2)\n",
    "print(\"Uczące:\", len(train_set), \", testowe:\", len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To proste rozwiązanie działa ale ! Każde nowe uruchomienie to nowy zbiór testowy. A to znaczy, że po krótkim czasie algorytmy będą widziały cały zbiór danych (znajdź info czym jest `data snooping bias`).\n",
    "\n",
    "W takim razie albo można zapisać testowy zbiór otrzymany za pierwszym razem i go później odtwarzać, albo można przed metodą `np.random.permutation()` dodać `np.random.seed(43)`. Można także dodać jakieś id (np metodą hash bądź stworzyć id na podstawie kolumn, które nigdy się nie zmieniają (np lon i lat). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podziel dane 20/80 wykorzystując moduł train_test_split z pakietu Scikit-Learn. \n",
    "\n",
    "nie zapomnij ustalić random_state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Od teraz zajmujemy się tylko zbiorem treningowym .\n",
    "\n",
    "Skopiuj zbiór treningowy do zmiennej `housing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wykreśl scatterplotem rozmieszczenie dystryktów \n",
    "\n",
    "wykres latitude od longitude\n",
    "\n",
    "na ramce housing użyj metody plot(kind='scatter', x=...,y=...,aplha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dodajmy jeszcze kolejne wymiary - koła=populacja, kolor=cena mieszkań\n",
    "\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4, s=housing[\"population\"]/100, label=\"Populacja\", figsize=(10,7), c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sprawdź korelacje \n",
    "\n",
    "Wartości współczynnika korelacji mieszczą się w zakresie pomiędzy –1 a 1. Wartości zbliżone do 1 wskazują silną korelację dodatnią. Z kolei wartości zbliżone do –1 mówią nam, że istnieje silna korelacja ujemna. Natomiast wartości bliskie zera oznaczają brak korelacji liniowej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.tools.plotting import scatter_matrix\n",
    "# ze względu na ilosc atrubutów ograniczymy widok do kilku cech\n",
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"]\n",
    "scatter_matrix(housing[attributes], figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Najbardziej obiecującym atrybutem służącym do prognozowania mediany cen mieszkań jest mediana dochodów, dlatego przyjrzyjmy się uważniej ich wykresowi korelacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\", alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Czy wszystkie zmienne są istotne ? \n",
    "\n",
    "Czy całkowita liczba pomieszczeń w dystrykcie jest wartościowym atrybutem ? a może lepszym byłoby stworzenie nowych zmiennych ?\n",
    "\n",
    "1. pokoje_na_rodzine = total_rooms/households\n",
    "\n",
    "2. sypialnie_na_pokoje = total_bedrooms/total_rooms\n",
    "\n",
    "3. populacja_na_rodzine = population/households\n",
    "\n",
    "Jaka jest korelacja nowych zmiennych ? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przygotowanie danych pod algorytmy\n",
    "\n",
    "Najlepiej zawsze pisać funkcje do przetwarzania bo:\n",
    "\n",
    "1. łatwo odtworzyć proces na nowych (świeżych) danych\n",
    "2. bibliotekę funkcji można wykorzystać w nowych projektach\n",
    "3. łatwość modyfikacji i wykorzystania do sprawdzania\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## wrocmy do pierwotnego zbioru treningowego i oddzielmy wyniki od zmiennych\n",
    "\n",
    "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Braki danych \n",
    "housing.dropna(subset=['total_bedrooms']) # 1. usunie braki danych\n",
    "housing.drop('total_bedrooms', axis=1) # 2. usunie całą kolumnę\n",
    "median = housing['total_bedrooms'].median() # 3. obliczy mediane\n",
    "housing['total_bedrooms'].fillna(median, inplace=True)# wstawi w brak mediane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W Sklearn.preprocessing istnieje moduł `Imputer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer(strategy='median')\n",
    "# dziala tylko na numericach\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
    "imputer.fit(housing_num)\n",
    "\n",
    "# porownaj\n",
    "imputer.statistics_\n",
    "# oraz \n",
    "housing_num.median().values\n",
    "\n",
    "# zastosowanie imputera\n",
    "X = imputer.transform(housing_num)\n",
    "# do ramki\n",
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
